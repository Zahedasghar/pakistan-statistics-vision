---
title: "The Case for Blended Data"
---

The most significant shift in how national statistics can be produced in the 21st century is the move toward blended data. This means combining information from multiple sources — surveys, censuses, administrative records, private sector data, and digital data to be used for statistical purposes. This blending produces statistical outputs that are more timely, more detailed, and more accurate than any single source could provide on its own. The idea is not new in principle. Statistical agencies have always supplemented one data source with another when convenient. What is new is the scale at which this is now both possible and necessary. 

The National Academies of Sciences, Engineering, and Medicine captured this shift in their report on 21st century data infrastructure, arguing that "careful blending of data from multiple, complementary sources, such as statistical surveys and censuses, administrative agencies, and the private sector, offers a way to generate more detailed, timely, and useful statistical information than is currently available" (NASEM, 2023, p. 27). This is not merely a technical observation. It is a recognition that the traditional model — in which statistical agencies design, collect, and process their own data in relative isolation — is no longer sufficient to meet the information demands of modern societies. 

For Pakistan, where the statistical system faces severe constraints of budget, coverage, and timeliness, the case for blended data is need of the hour. Nevertheless, it has difficulties to execute. The infrastructure, legal frameworks, and institutional work ethics needed to blend data from diverse sources are not in place at the moment. This chapter examines why blended data matters, what it requires, and what must change for it to work. 


## Why No Single Source Is Enough

Each data source has its own weaknesses, and these weaknesses are well documented in the statistical literature. Understanding them is the starting point for understanding why blending is necessary. 
Surveys have historically been the backbone of national statistics. PSLM, HIES, LFS among others provide structured, well-documented data collected using probability sampling methods. But surveys are under pressure everywhere. Response rates have been declining for decades across countries. Though response rate reported in recently released HIES is very encouraging yet many countries are facing high drop in response rate. The U.S. Current Population Survey, for instance, saw its response rate drop from over 95 per cent in the 1990s to below 85 per cent by the late 2010s. The American Community Survey fell from around 97 per cent to approximately 86 per cent over a similar period (NASEM, 2023, Table 2-1). Pakistan's household surveys face challenges — security constraints in certain regions, limited field staff, poor infrastructure in remote areas, and respondent fatigue all contribute to growing nonresponse. Groves (2011) documented how survey research globally had entered what he called a "third era" characterised by falling participation, rising costs, and increasing reliance on supplementary sources. 

Beyond response rates, surveys are expensive and slow. A typical household survey in Pakistan takes 12 to 18 months from fieldwork to publication, which limits its usefulness for policy decisions that require timely data. Sample sizes also impose limits on the granularity of estimates — most surveys cannot produce reliable district-level statistics for important indicators like poverty or unemployment. And as the NASEM panel noted, even when surveys achieve full coverage, they "are expensive, slow to produce information, and suffer from nonparticipation" (NASEM, 2023, p. 27). 

 
Administrative records offer the advantage of large coverage and low marginal cost, since the data already exists as a byproduct of government operations. Tax records, social protection registries, civil registration, health facility records — these data cover large populations and are updated regularly.
But they were not designed for statistical purposes. Wallgren and Wallgren (2007) emphasised this fundamental tension: administrative data reflects "administrative reality, not statistical reality." The definitions used in a tax system or a social programme may not match the concepts that statistical agencies need to measure. Coverage may be incomplete — Pakistan's tax base, for example, covers only a fraction of the working population. Data quality depends on the administrative process that generates it, which may vary across regions, change over time, or be affected by fraud and underreporting. 

The U.S. experience illustrates both the promise and the limitations. The Census Bureau has used federal tax data in its economic censuses since the 1950s and in building its business register since the early 1970s (NASEM, 2023, p. 77). Yet even in that mature system, legal restrictions prevent the sharing of certain tax data between statistical agencies. The Internal Revenue Code still blocks the Census Bureau from sharing business tax data with the Bureau of Labor Statistics and the Bureau of Economic Analysis, despite repeated legislative attempts since 2002 to change this (NASEM, 2023, p. 78). If the United States, with its well-resourced institutional infrastructure, struggles with these barriers, the challenges for Pakistan are likely to be even more significant. 

Private sector data — from telecom companies, banks, digital platforms, retailers — can be extraordinarily timely and granular. Transaction data can provide near-real-time indicators of economic activity. Mobile phone metadata has been shown to predict poverty levels at fine spatial resolution (Blumenstock, Cadamuro, & On, 2015). Scanner data from supermarkets has been used in the Netherlands to compile consumer price indices with greater accuracy and lower respondent burden than traditional price collection methods (Chessa, 2016). In the United States, all 13 designated federal statistical agencies except one were using private sector data assets at the time of the NASEM report, with the Bureau of Economic Analysis alone using some 142 private sector data sources (NASEM, 2023, p. 28). 

But private sector data comes with serious complications. Companies have legitimate concerns about commercial confidentiality. Data quality is often unknown — as one workshop participant told the NASEM panel, private sector data is not like "gold dust" but rather like "sand, abundant and requiring significant effort to make them useful" (NASEM, 2023, p. 94). The data may not be representative of the population. Coverage depends on who uses the service, which typically skews toward wealthier, younger, and more urban populations. Data definitions can change without notice when companies update their systems. And data-use agreements tend to be one-off arrangements with no inherent sustainability (NASEM, 2023, p. 29). 

Digital and emerging data sources — satellite imagery, social media, crowdsourced data, sensor data — add further possibilities but also further complications. These sources can be large in volume but noisy and difficult to interpret. They often lack the metadata needed to assess quality or fitness for statistical use. 

No single source resolves all these weakness. But when carefully combined, different sources can compensate for each other's limitations. Survey data provide the statistical framework, quality controls, and conceptual definitions. Administrative data fill coverage gaps and reduce respondent burden. Private sector data add timeliness and granularity. The result is a richer, more complete picture — one that none of these sources could produce alone. 


## What Blended Data Requires

Recognising the value of blended data is one thing. Actually producing it is quite another. The technical, institutional, and governance requirements are substantial, and they cut across multiple dimensions.

### New Statistical Methods

Combining data from different sources is not straightforward, and it cannot be done by simply appending one dataset to another. Each source has its own coverage, its own definitions, its own error structure, and its own biases. Bringing them together requires a set of statistical methods that were not part of the traditional survey statistician's toolkit. 

Lohr and Raghunathan (2017), in their comprehensive review in Statistical Science, identified four broad families of methods for combining data from multiple sources. The first is record linkage — matching individual records across datasets using identifying information like names, identity numbers, or addresses. This is the most direct form of data integration and has been used extensively in countries with well-developed statistical infrastructure. But as the NASEM panel noted, "linkage rates vary across studies and for subpopulations within studies" (NASEM, 2023, p. 96), and linkage requires that individual-level data with sufficient identifying information be available from each source. In Pakistan, where the CNIC number could serve as a universal linkage key, the potential for record linkage is considerable — but realising that potential depends on legal access to the relevant datasets.
The second family is multiple frame methods, in which independent samples from different sampling frames are combined to improve coverage or reduce costs. This is a technique with a long history in survey statistics (Lohr, 2021) and can be adapted to the multi-source environment by treating administrative registers or private datasets as additional frames. 

Third, imputation-based methods treat the problem of combining sources as a missing data problem. Variables that are available in one dataset but not another can be imputed using statistical models that exploit the relationships among observed variables. Lohr and Raghunathan (2017) note that while imputation provides a transparent framework for combining incompatible sources, it requires considerable expertise and careful attention to the differences between data sources — differences in respondent types, interview modes, survey contexts, and measurement approaches. 

Fourth, modelling techniques — including small area estimation, Bayesian hierarchical models, and machine learning approaches — can be used when direct linkage or imputation is not possible or appropriate. These methods have been used, for example, to combine satellite imagery with survey data to predict poverty at fine spatial resolution (Jean et al., 2016). 

The NASEM report recommended that statistical systems "systematically coordinate agencies' efforts to blend multiple data sources" and "ensure that statistical agencies have the appropriate skills and expertise" for these methods (NASEM, 2023, p. 97). For Pakistan, where methodological capacity within PBS and provincial bureaus is limited, investing in these skills is not optional — it is a prerequisite for any meaningful use of blended data. 


### New Statistical Designs

Once blended data become available, the design of statistical programmes itself can change. The traditional approach — in which a single survey is designed to capture all needed information — gives way to an approach in which different sources are assigned different roles, and survey designs are optimised to complement what other sources already provide. 

This is perhaps the most exciting implication of blended data, and the one least discussed. As the NASEM panel put it, "after new blended statistics using multiple data sources are built, survey designs could likely be optimised, reducing original survey measurement in populations that are well measured and increasing survey measurement in populations not well covered by the various administrative record systems" (NASEM, 2023, p. 97). In practical term, this could mean smaller but more targeted surveys, focused on filling the specific gaps that administrative and private sector data cannot cover.
The U.S. Census Bureau's modernisation of its residential construction statistics programme illustrates this approach. Rather than collecting housing permit data from 9,000 permit-issuing organisations, the Bureau now receives data from third-party sources and supplements it with a small cutoff sample. Satellite imagery is used to identify construction starts instead of telephone interviews. The result is more granular statistics — permit data for every jurisdiction rather than just states — at lower cost (NASEM, 2023, p. 31). 

For Pakistan, the implication is significant. The country cannot afford large-scale surveys at the frequency and granularity that modern policymaking demands. But if administrative data from agencies like NADRA, FBR, SECP, NEPRA, PEMRA  and BISP can be systematically accessed and combined with periodic surveys, the surveys themselves can be redesigned to be smaller, cheaper, and more focused — while the blended output is more comprehensive than either source alone.

### New Capabilities

The NASEM panel drew on work by the United Nations Economic Commission for Europe (UNECE) to identify a set of capabilities that a modern data infrastructure would need for blending (NASEM, 2023, Box 4-4). These include the familiar ones — data transformation, security, and governance — but also several that are novel for many statistical agencies. 

Data design, definition, and description for data not originally built for statistical analysis is perhaps the most critical capability gap. Administrative and private sector data typically lack the kind of metadata documentation that survey datasets carry. Variables may not be clearly defined.
Coverage boundaries may not be documented. Without adequate metadata, meaningful blending is impossible. The NASEM panel was emphatic on this point: "to be responsibly discovered, combined, shared, used, and reused, data must be described. Limitations of data must also be readily accessible to ensure that biases in individual data assets do not ripple through any analysis" (NASEM, 2023, p. 94). 

Data logistics — managing the supply chain of data from holders to users and back — is another capability that statistical agencies typically lack. In the 20th century, statistical agencies designed and collected their own data. The data sat behind the agency's firewall, under the agency's full control. In a blended data world, agencies must negotiate access to datasets held by other organisations, often on an ongoing basis, under conditions that respect the data holder's interests. This is fundamentally different from traditional data collection. 

Data integration — the ability to link, combine, and align datasets from multiple sources — requires not just technical tools but also deep substantive understanding of what each dataset represents and where its limitations lie. This includes what the NASEM panel called knowledge management: documenting the meaning of individual measurements across diverse data assets so that analysts understand what they are combining (NASEM, 2023, p. 98). 

The panel's assessment was blunt: "such data silos no longer serve the needs of modern society. Features of the 20th century data infrastructure must change" (NASEM, 2023, p. 98). This is equally true for Pakistan's statistical system, where data integration capabilities are still minimal across most agencies.

### New Quality Frameworks

Blending data from multiple sources does not just add complexity — it fundamentally changes how quality must be assessed. Conventionl quality frameworks were designed for single-source data, typically surveys, where concepts like sampling error, nonresponse bias, and measurement error had well-understood definitions and estimation methods. When multiple sources are combined, the quality of the blended output depends on the quality of each input, the method used to combine them, and the fitness of the result for its intended purpose. 

The U.S. Federal Committee on Statistical Methodology (FCSM) addressed this challenge in its 2020 report, A Framework for Data Quality, which defined quality as "the degree to which data captures the desired information using appropriate methodology in a manner that sustains public trust" (FCSM, 2020, p. 6). The framework identifies 11 dimensions of data quality grouped across three domains: utility (relevance, accessibility, timeliness, punctuality, granularity), objectivity (accuracy, coherence, comparability), and integrity (credibility, transparency, confidentiality). Importantly, the framework applies to all data types — surveys, administrative records, blended data, and emerging sources — and emphasises fitness-for-purpose rather than adherence to any single standard of accuracy. 

This is directly relevant for Pakistan. Any move toward blended data will require the development of quality assessment protocols that go beyond traditional survey error frameworks. The quality of an administrative dataset from NADRA or FBR cannot be evaluated using the same metrics as a probability survey. Different questions must be asked — about coverage completeness, definitional consistency, update frequency, and processing integrity. And when such data is combined with survey data, the quality of the blend must be assessed as a whole, not just source by source.

### New Privacy Safeguards 

Blending data from multiple sources increases the potential for re-identification of individuals. A dataset that is anonymised on its own may become identifiable when linked with another dataset. This is a well-known risk in the privacy literature, and it becomes more acute as the number and diversity of data sources increase. 

The NASEM panel devoted considerable attention to this challenge, noting that "a 21st century national data infrastructure cannot succeed without ensuring ethical exchange of data; trust in institutions involved in data exchange; privacy-preserving techniques; and technical, organisational, and legal mechanisms supporting responsible data practices" (NASEM, 2023, p. 99). The panel identified four ethical values that must underlie data infrastructure: attention to how use of a subject's data affects their life; respect for autonomy and informed consent; concern for beneficence; and respect for human dignity.
On the technical side, advances in privacy-enhancing technologies offer new possibilities. Differential privacy, synthetic data generation, secure multiparty computation, and homomorphic encryption are all being explored in various countries. The U.S. Census Bureau began releasing data using synthetic data generation in 2006 as part of its Longitudinal Employer-Household Dynamics programme (NASEM, 2023, p. 101). Several U.S. agencies including NIH, NIST, and NSF are working on standards for homomorphic encryption to enable computation on encrypted data. 

For Pakistan, where public trust in government handling of personal data is limited and legal frameworks for data protection are still developing, these safeguards are not secondary concerns — they are preconditions. Without credible privacy protections, neither citizens nor private sector data holders will support the data sharing that blended statistics require. The "Five Safes" framework (Desai, Ritchie, & Welpton, 2016) — safe projects, safe people, safe settings, safe data and safe outputs offers a practical model for structuring access controls, one that several countries including the UK and Australia have adopted with success.

### International Experience and Lessons

The move toward blended data is not happening in a vacuum. Many countries are already well advanced in this direction, and their experiences offer useful lessons for Pakistan. Statistics Canada has adopted an "administrative data first" policy, meaning that it seeks to use existing administrative records before resorting to new data collection (NASEM, 2023, p. 80). Canada also developed the necessity and proportionality criteria for data intake — acquiring no more data than needed for the specified statistical purpose and considering the sensitivity and confidentiality of the data (Bowlby, 2021). This disciplined approach avoids the unbridled harvesting of all available data and focuses resources where they are most needed. 

Statistics Netherlands has been a pioneer in using private sector data for official statistics. Its work on scanner data for consumer price indices, led by researchers like Chessa (2016), demonstrated that electronic transaction data from supermarkets could replace much of the traditional manual price collection. The Dutch approach shows that private sector data can not only supplement but in some cases improve upon traditional methods — but only after considerable investment in methodology and data quality assessment. 

The United Kingdom created the UK Statistics Authority as an independent oversight body responsible for promoting and safeguarding official statistics that "serve the public good" (NASEM, 2023, p. 59). This kind of institutional accountability is important for building public trust when statistical systems move toward using more diverse and sensitive data sources. 

For Pakistan, the lesson from international experience is not that it should replicate any single country's approach. Legal frameworks, institutional capacities, and data landscapes differ too much for that. The lesson is rather that blended data is not an abstract aspiration. It is a practical reality in many countries. Pakistan can start working on through a combination of legal reform, institutional investment, methodological development, and sustained partnership between statistical agencies and data holders. Pakistan can learn from the experiences of other countries but  it has to develop local capacity as its not a one-off process but an on ongoing scientific mechanism. So adapting in own context is important.
 
## What Blended Data Does Not Mean

A few clarifications are important to avoid misunderstanding. Blended data does not mean the end of surveys. Surveys will remain essential for measuring concepts that cannot be captured through administrative records or transactions. Administrative data may not be suitable for capturing attitudes, perceptions, informal economic activity, unpaid care work, and many other topics. What changes is the role of surveys: from being the sole source to being one component in a multi-source system. As the NASEM panel recommended, blended data should lead to the redesign of surveys, not their abandonment. 

Blended data also does not mean the unrestricted harvesting of all available digital data. The NASEM panel was explicit that "a new data infrastructure should not result in the unbridled harvesting of all digital data that exists in the country" (NASEM, 2023, p. 91). Data acquisition should be guided by necessity — the data must serve a pre-specified statistical purpose — and proportionality — the amount and detail of data acquired should be limited to what that purpose requires. 

Finally, blended data does not reduce the need for quality assessment. If anything, it increases it. Each input source must be evaluated on its own terms, and the blended output must be assessed for fitness-for-purpose. Without rigorous quality frameworks, blended data risks producing statistics that appear comprehensive but rest on unexamined foundations. 



::: {.callout-warning}
## A Critical Shift
The NASEM panel's assessment is direct: "such data silos no longer serve the needs of modern society" (NASEM, 2023, p. 98). The features of 20th century data systems must change, and this demand enhanced capabilities across the board — in methods, technology, governance, and privacy protection. For Pakistan, where the statistical system still operate largely in the traditional single-source model, this shift represents both the biggest challenge and the biggest opportunity.
:::


## References

Blumenstock, J., Cadamuro, G., & On, R. (2015). Predicting poverty and wealth from mobile phone metadata. *Science*, 350(6264), 1073–1076.

Bowlby, G. (2021). Private sector administrative data and the Canadian statistical system. Presentation to the National Academies' Panel on the Scope, Components, and Key Characteristics of a 21st Century Data Infrastructure, December 9, 2021.

Chessa, A. G. (2016). A new methodology for processing scanner data in the Dutch CPI. *Eurostat Review on National Accounts and Macroeconomic Indicators*, 1/2016, 49–69.

Desai, T., Ritchie, F., & Welpton, R. (2016). Five Safes: Designing data access for research. Working Paper, University of the West of England.

Federal Committee on Statistical Methodology. (2020). *A Framework for Data Quality*. FCSM-20-04. September 2020.

Groves, R. M. (2011). Three eras of survey research. *Public Opinion Quarterly*, 75(5), 861–871.

Jean, N., Burke, M., Xie, M., Davis, W. M., Lobell, D. B., & Ermon, S. (2016). Combining satellite imagery and machine learning to predict poverty. *Science*, 353(6301), 790–794.

Lohr, S. L. (2021). Multiple-frame surveys for a multiple-data-source world. *Survey Methodology*, 47(2), 229–263.

Lohr, S. L., & Raghunathan, T. E. (2017). Combining survey data with other data sources. *Statistical Science*, 32(2), 293–312.

National Academies of Sciences, Engineering, and Medicine. (2017). *Federal Statistics, Multiple Data Sources, and Privacy Protection: Next Steps*. Washington, DC: The National Academies Press.

National Academies of Sciences, Engineering, and Medicine. (2023). *Toward a 21st Century National Data Infrastructure: Mobilizing Information for the Common Good*. Washington, DC: The National Academies Press. https://doi.org/10.17226/26688.

Wallgren, A., & Wallgren, B. (2007). *Register-Based Statistics: Administrative Data for Statistical Purposes*. Chichester: John Wiley & Sons.