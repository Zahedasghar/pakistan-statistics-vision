# Preparing for the Future: AI-Ready Data

It would be a mistake — and not a small one — to design a new data infrastructure that serves only today's analytical needs. The world is changing rapidly in how data is produced, managed, and used, and the countries that fail to anticipate these changes will find themselves building systems that are already outdated by the time they become operational. Artificial intelligence and machine learning are not futuristic possibilities. They are present realities, transforming everything from how national accounts are compiled to how satellite imagery is classified, from how survey non-response is imputed to how administrative records are linked across agencies. None of these applications would be possible without high-quality, well-structured, properly documented data. The infrastructure Pakistan builds now must therefore be designed not merely for traditional statistical tabulation but for a future in which advanced computational methods are the standard tools of evidence production.

This does not mean that Pakistan should rush to adopt every fashionable technology. It means something more fundamental: that the basic properties of data — its structure, its documentation, its consistency, its accessibility — must meet standards that enable not only human analysts but also machines to find, read, interpret, and use it. Getting these foundations right is the most important investment the country can make. Without them, even the most sophisticated algorithms will produce unreliable results, and the promise of AI for public policy will remain unfulfilled.

## The Gap Between Collecting Data and Making It Usable

Pakistan collects an enormous amount of data. NADRA processes biometric records for over 200 million citizens. The District Health Information System (DHIS2) captures data from thousands of health facilities across the country. The National Education Management Information System (NEMIS) tracks school enrolments and teacher deployments. The Federal Board of Revenue hold tax records. BISP maintains records on millions of beneficiary households. The Pakistan Bureau of Statistics conducts censuses, labour force surveys, household income and expenditure surveys, and a range of other data collection exercises.

But collecting data is not the same as making it usable. Much of Pakistan's existing data exists in formats that are difficult to access, hard to combine with other sources, and poorly documented. Survey microdata files are sometimes released without complete data dictionaries or codebooks. Administrative records follow agency-specific coding systems that are incompatible with one another. Metadata — the descriptive information that tells a user what each variable means, how it was measured, when it was collected, and what limitations it carries — is frequently incomplete or altogether absent. A dataset without proper metadata is like a library book without a catalogue entry: it may contain valuable information, but no one can find it, and even those who stumble upon it cannot easily determine whether it is relevant to their needs or how to interpret it correctly.

This problem is not unique to Pakistan. Across the world, researchers report spending up to 80 percent of their time preparing data into usable formats before any analysis can begin (Bipartisan Policy Center, 2022). The gap between raw data and analytical-ready data is enormous, and bridging it consumes resources that could otherwise be devoted to producing insights. In developing countries, where technical capacity is already scarce, this inefficiency is especially costly. The infrastructure Pakistan builds must therefore treat data preparation, documentation, and quality assurance not as afterthoughts but as core functions — as essential as data collection itself.

## What FAIR Actually Requires

The international standard for making data usable is captured in the FAIR principles, first articulated by Wilkinson and colleagues in 2016 and since adopted by research funders, statistical agencies, and international organisations around the world. FAIR stands for Findable, Accessible, Interoperable, and Reusable. Each principle addresses a specific barrier to data use.

Findable means that datasets must be registered in searchable catalogues with rich, standardised metadata, and that each dataset must have a unique, persistent identifier — the digital equivalent of an ISBN for a book. Without this, users cannot discover what data exists. In Pakistan today, there is no comprehensive catalogue of the data held by different government agencies. A researcher wanting to know what education data is available, at what geographic level, for what time periods, and with what variables, would need to contact individual agencies and hope for a response. A national data catalogue — a searchable register of all major datasets held by government, with standardised descriptions of their content, coverage, and access conditions — would be a relatively low-cost, high-impact first step.

Accessible means that once a user finds a dataset, there should be a clear, documented process for obtaining access to it. This does not mean that all data must be openly downloadable. Some data — particularly individual-level administrative records — must be restricted for privacy reasons. But the access procedures must be transparent and standardised, not ad hoc. Users should know what data is available, what conditions apply to its use, and how to apply for access. The current situation in Pakistan, where access to government data often depend on personal connections and informal negotiations rather than on published procedures, is neither efficient nor equitable.

Interoperable means that data from different sources must be able to work together. This requires common coding standards, shared classification systems, and compatible file formats. If DHIS2 records a patient's district of residence using one coding scheme and NADRA uses a different one, linking the two datasets becomes a labour-intensive exercise in manual reconciliation. If PBS uses one definition of "urban" and provincial planning departments use another, combining their data produces confusion rather than clarity. Interoperability is not achieved by decree. It requires sustained technical work to develop and maintain common standards, and institutional commitment to adopting them consistently across agencies. The Statistical Data and Metadata Exchange (SDMX) standard, developed by a consortium of international organisations including the IMF, World Bank, and OECD, provides one framework for achieving interoperability in statistical data. Pakistan's adoption of SDMX standards — already recommended in various reform proposals — would be a significant step toward making its data infrastructure interoperable.

Reusable means that data must be well-documented enough that it can be understood, replicated, and used in new contexts by people who were not involved in its original collection. This requires detailed metadata describing how variables were defined, how samples were drawn, what quality checks were applied, and what known limitations the data carries. It also require clear licensing that specifies what users are permitted to do with the data. In many countries, including Pakistan, the legal status of government data — who owns it, who can use it, under what conditions — is ambiguous. This ambiguity discourages reuse and forces potential users to navigate a fog of uncertainty that many, especially those outside government, simply choose to avoid.

It is worth emphasising that FAIR does not mean "open." The FAIR principles are sometimes conflated with the open data movement, but they are conceptually distinct. A dataset can be FAIR — well-documented, discoverable, structured, and accessible through a transparent process — without being publicly downloadable. This distinction is critically important for administrative data, which often contains sensitive individual-level information that cannot and should not be released publicly. The FAIR framework accommodates restricted access: what it requires is that the *conditions* of access are clear and standardised, not that access is unrestricted. For Pakistan, this means that applying FAIR principles to NADRA records, tax data, or health facility records does not require making them public. It requires documenting what the data contains, establishing who can access it, under what governance procedures, and ensuring that the data is structured in ways that permit authorised linkage and analysis.

## Why AI Changes the Stakes

The FAIR principles were developed primarily with human researchers in mind, but they have become even more important in the age of artificial intelligence. Machine learning systems are powerful pattern-recognition tools, but they are also unforgiving consumers of data. A human analyst can often work around inconsistencies in a dataset — recognising, for instance, that "Faisalabad" and "Lyallpur" refer to the same city, or that a column labelled "inc" probably means "income." A machine cannot make these inferences unless the data is structured and documented in ways that eliminate ambiguity.

This is what "AI-ready" means in practice: data that is not only high-quality by traditional statistical standards but also structured for machine consumption. The UK government's 2025 guidelines on making government datasets AI-ready identify four pillars: technical optimisation (data is structured and formatted for efficient machine use); data and metadata quality (data is accurate, complete, consistent, and maintained through effective processes); organisational and infrastructure context (data governance and stewardship arrangements are in place); and legal, security, and ethical compliance (data use is aligned with applicable laws and managed responsibly) (DSIT, 2025). These four pillars are not separate from good statistical practice. They are the same principles that make data useful for any purpose, carried to the standard of consistency and documentation that machines require.

For Pakistan, the practical implications are significant. Consider just one example: the potential use of machine learning to improve the targeting of social protection programmes. BISP currently uses a proxy means test based on household survey data to determine eligibility for cash transfers. Machine learning methods could potentially improve targeting accuracy by combining survey data with administrative records from multiple sources — tax data, utility consumption, land ownership records, school enrolment — to build more nuanced predictions of household welfare. But this would require all of these data sources to be structured in compatible formats, documented with consistent metadata, linked through common identifiers, and accessible through governed procedures. Without these foundations, the machine learning application is not feasible, regardless of how sophisticated the algorithm might be.

The same logic applies to other domains. Predictive analytics for disease surveillance using DHIS2 data. Satellite imagery classification for agricultural crop monitoring. Natural language processing of court records for justice system analysis. Text mining of parliamentary proceedings. In every case, the quality, structure, and documentation of the underlying data determine whether the analytical method can produce reliable results. The data infrastructure is the binding constraint, not the algorithm.

## The National Data Library as Institutional Architecture

The concept of a National Data Library has gained significant momentum internationally, most prominently in the United Kingdom, where the government announced plans in 2024 to create such an institution. The UK's National Data Library, as envisioned by the Department for Science, Innovation and Technology (DSIT), is not a single centralised database. It is a service layer — a governed infrastructure that enables curated, de-identified, research-ready datasets to be discoverable, accessible, and linkable within secure environments (GOV.UK, 2025). The Tony Blair Institute estimated in 2025 that a fully developed National Data Library could generate returns of £5 for every £1 invested in data linkage, with wider societal benefits potentially reaching £319 billion by 2050 (TBI, 2025).

The UK model builds on the work of Administrative Data Research UK (ADR UK), which since 2018 has invested over £105 million in creating secure access to linked administrative data for research. A key insight from ADR UK's experience is that the traditional "create and destroy" model of data access — where each research project negotiates its own access to raw data, conducts its own linkage, and destroys the linked dataset after use — is enormously inefficient. The ADR UK approach instead does the governance, cleaning, and linkage work upfront, so that de-identified, research-ready, curated datasets can be maintained over time and accessed by multiple approved researchers. This allows knowledge to accumulate: researchers can share code, derived variables, and analytical methods, building on what has come before rather than starting from scratch each time (ADR UK, 2024).

For Pakistan, the National Data Library concept is worth adapting to local conditions. The country does not need to build a replica of the UK system. It need a mechanism that performs several essential functions: cataloguing what data exists across government; establishing and enforcing common standards for data structure, documentation, and quality; creating governed procedures for data access that are transparent, consistent, and proportionate to the sensitivity of the data; providing a secure environment for approved research using linked administrative data; and building institutional capacity for data curation and stewardship.

This last function — capacity for data curation — is perhaps the most neglected and most important. Data does not curate itself. Converting raw administrative records into research-ready datasets requires skilled professionals who understand both the domain (health, education, taxation) and the technical requirements of data management. These skills are in short supply in Pakistan, and the infrastructure will not function without sustained investment in building them. A National Data Library is not primarily a technology project. It is an institutional project — one that requires people, governance structures, and sustained funding at least as much as it require servers and software.

## From Static Systems to Adaptive Infrastructure

A data infrastructure designed only for the analytical methods of today will be inadequate for the methods of tomorrow. The pace of technological change means that new types of data, new sources, and new analytical approaches will emerge continuously. An infrastructure that cannot accommodate these changes will quickly become obsolete.

Several developments illustrate what an adaptive infrastructure must prepare for. First, the growing availability of geospatial data — satellite imagery, GPS traces, mobile phone location data — offers enormous potential for understanding economic activity, agricultural production, urbanisation patterns, and environmental change. Pakistan's agricultural sector, which employs a large share of the workforce and is increasingly affected by climate variability, could benefit enormously from the integration of satellite-derived crop estimates with ground-level agricultural survey data. But this requires the infrastructure to handle raster data, vector data, and tabular data within a common framework.

Second, the proliferation of real-time and high-frequency data streams — from mobile phone usage, digital payment systems, web traffic, and sensor networks — creates opportunities for more timely indicators of economic and social conditions. During the COVID-19 pandemic, countries that could rapidly deploy alternative data sources to supplement traditional surveys were better positioned to monitor the crisis in near real-time. Pakistan's infrastructure should be designed to incorporate such data sources as they become available, even if their integration is not immediate.

Third, the development of privacy-preserving analytical techniques — including differential privacy, federated learning, and secure multi-party computation — offers new ways to extract insights from sensitive data without exposing individual records. Federated learning, for example, allows machine learning models to be trained on data distributed across multiple institutions without the data ever leaving its original location. This could be particularly valuable in Pakistan, where institutional reluctance to share raw data is a major barrier to integration. If the infrastructure supports federated approaches, agencies can contribute to analytical products without surrendering control of their data.

Fourth, the emergence of large language models and other generative AI tools creates new possibilities for working with unstructured data — text, images, audio — that have traditionally been outside the scope of statistical infrastructure. Court records, parliamentary debates, citizen complaints, and media coverage all contain information relevant to policy analysis. An infrastructure that can accommodate unstructured data alongside traditional structured datasets will be significantly more valuable than one that cannot.

None of these developments require Pakistan to adopt them immediately. They require that the infrastructure be designed with sufficient flexibility to accommodate them as they mature and as the country's technical capacity grows. This means adopting open standards rather than proprietary formats, modular architectures rather than monolithic systems, and governance frameworks that can evolve with changing technology and changing needs.

The temptation in any large infrastructure project is to build for the specifications of the moment. A system designed in 2025 to process the data types and volumes that exist in 2025 will struggle when confronted with the data types and volumes of 2030. The infrastructure should therefore be built on the principle that change is not an exception to be managed but a constant to be expected. Application programming interfaces (APIs) should be designed to accommodate new data sources without requiring re-architecture. Data storage systems should handle structured, semi-structured, and unstructured data. Governance protocols should be flexible enough to apply to data types that have not yet been imagined. This is not a call for unlimited spending on future-proofing. It is a call for architectural decisions that prioritise openness and modularity over closure and rigidity.

## The Political Economy of Data as a Public Asset

There is one final argument that must be made, and it is not a technical one. Data produced through the use of public resources — through government surveys, administrative processes, and publicly funded research — is a public asset. It belong to the citizens whose taxes funded its collection and whose information it contains. Treating it as such has implications for how the infrastructure is governed.

In too many countries, including Pakistan, government data is treated as the property of the agency that collected it. Agencies are reluctant to share their data with other agencies, let alone with researchers or the public. This reluctance is sometimes justified by legitimate concerns about privacy, quality, or misinterpretation. But it is often driven by institutional culture — a sense of ownership over "my data" — or by bureaucratic inertia. The result is that data collected at public expense sits unused in agency silos, generating no value beyond the narrow purpose for which it was originally collected.

Overcoming this requires a shift in mindset that treats data sharing as the default rather than the exception, subject to appropriate safeguards for privacy and confidentiality. The UK's Digital Economy Act 2017 established a legal framework for sharing government data for research purposes, with specific provisions requiring that shared data be used for the "public good." Similar legislation in Pakistan — establishing a legal obligation to share administrative data for statistical and research purposes, with appropriate governance mechanisms — would provide the institutional foundation for a more productive data ecosystem.

The World Bank's Development Data Group has articulated this vision in terms of "AI-ready development data," arguing that the challenge is not a scarcity of high-quality data but rather the absence of standardised frameworks and infrastructure to make existing data consistently findable, accessible, and usable (World Bank, 2025). The same argument applies to Pakistan. The data exists. The gap is in the infrastructure — technical, institutional, and legal — that would make it useful. Building that infrastructure is not merely a technical modernisation exercise. It is an investment in the country's capacity to understand itself, to make evidence-based decisions, and to participate in the global economy of knowledge and innovation.

The decisions made in the next few years about how Pakistan's data infrastructure is designed will shape the country's analytical capacity for decades. If the infrastructure is built on open standards, with strong governance, comprehensive metadata, and the flexibility to accommodate new data sources and methods, it will serve as a foundation for continuous improvement. If it is built as a rigid, closed, and poorly documented system, it will become another legacy burden — expensive to maintain, difficult to adapt, and ultimately unable to deliver the evidence that the country needs. The choice is not between a perfect system and no system. It is between a system designed to learn and grow, and one that is not. Pakistan should choose the former.

## References

ADR UK (2024). The new UK Government wants a National Data Library: a brilliant aspiration, if built on solid foundations. London: Administrative Data Research UK.

Bipartisan Policy Center (2022). AI-Ready Open Data. Washington, DC: Bipartisan Policy Center.

DSIT (2025). Guidelines and best practices for making government datasets ready for AI. London: Department for Science, Innovation and Technology.

GOV.UK (2025). National Data Library. London: HM Government.

TBI (2025). Governing in the Age of AI: Building Britain's National Data Library. London: Tony Blair Institute for Global Change.

UK Digital Economy Act (2017). *Digital Economy Act 2017*. London: Her Majesty's Stationery Office.

Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., da Silva Santos, L. B., Bourne, P. E. and others (2016). The FAIR Guiding Principles for scientific data management and stewardship. *Scientific Data* 3, 160018.

World Bank (2025). From Open Data to AI-Ready Data: Building the Foundations for Responsible AI in Development. Washington, DC: World Bank Development Data Group.